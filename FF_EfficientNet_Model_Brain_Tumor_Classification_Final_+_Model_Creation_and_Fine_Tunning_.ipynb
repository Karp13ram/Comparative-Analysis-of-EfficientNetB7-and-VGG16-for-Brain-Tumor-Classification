{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clrGUGuExT_o"
   },
   "source": [
    "## The Efficientnet model development and fine tuning\n",
    "\n",
    "in this notebook we will load and add our layers to the base model of the pre-trained EfficientnetB7 model, and fine tune the following parameters of the model:\n",
    "1. **Optimizers** - We will use only 5 optimizers to see which one works best, namely; 1. `SGD`,2. `RMSprop`,3. `Adam`,4. `Adagrad`,5. `Adadelta`. the best optimizer will be used in the New model\n",
    "2. **Number of epochs** - We will also use only 4 epochs to choose the best performer namely; `1, 2, 5, 10`.\n",
    "3. **Batch size** - we will also use these batch sizes to choose the optimum batch size, namely; `8, 16`. \n",
    "4. **Dropout** - we will optimise the dropout of the model, values `0.5, 0.6, 0.7, 0.8, 0.9`\n",
    "\n",
    "> **Section one:** we will create common function and also create our base model, with the following initial parameters: **Batch size** `8`, **Optimizer** `Adam`, **Number of epochs** `4`, **dropout value** `0.5`. \n",
    "\n",
    "# **Import libraries that we will use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4cjGKuypxT_t"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "import efficientnet.keras as efn \n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "import tqdm.gui as tqdm\n",
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U79EViinxT_v"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Activation, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sr5Vl1mxxT_x"
   },
   "source": [
    "# **Common functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T3mIQtpcxT_y"
   },
   "outputs": [],
   "source": [
    "# common functions that i will use\n",
    "\n",
    "def get_data_generator(train_data_path, \\\n",
    "                       val_data_path, \\\n",
    "                       targetsize, \\\n",
    "                       classmode, \\\n",
    "                       batchsize):\n",
    "    \"\"\"\n",
    "    This function is a data generator function for train, validation, and testing data\n",
    "    Inputs\n",
    "        train_data_path   : train data path for the dataset \n",
    "        val_data_path     : validation data path for the dataset\n",
    "        targetsize        : target size for the generator to resize all images to, (224,224)\n",
    "        classmode         : class mode, 'categorical'\n",
    "        batchsize         : batch size\n",
    "    Outputs\n",
    "        train_generator   : generated train data\n",
    "        val_generator     : generated validation data\n",
    "    \n",
    "    \"\"\"\n",
    "    train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    \n",
    "    \n",
    "    train_generator=train_datagen.flow_from_directory(train_data_path, # this is where you specify the path to the main data folder\n",
    "                                                 target_size=targetsize,\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=batchsize,\n",
    "                                                 class_mode=classmode,\n",
    "                                                 shuffle=True)\n",
    "    \n",
    "    val_generator=val_datagen.flow_from_directory(val_data_path, # this is where you specify the path to the main data folder\n",
    "                                                 target_size=targetsize,\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=batchsize,\n",
    "                                                 class_mode=classmode,)\n",
    " \n",
    "    return train_generator,val_generator   \n",
    "    \n",
    "\n",
    "\n",
    "def train_evaluate_the_model(train_generator, \\\n",
    "                             val_generator, \\\n",
    "                             optimizer, \\\n",
    "                             epochs, \\\n",
    "                             dropout_value, \\\n",
    "                             TheModel):\n",
    "    '''\n",
    "    train the model, do predictions, and do evaluation and return the accuracy of the model\n",
    "    Inputs\n",
    "        train_generator : generated train data\n",
    "        val_generator   : generated validation data\n",
    "        optimizer       : the optimizer method used to compile the model\n",
    "        epochs          : the epochs of the model use to fit the model\n",
    "        dropout_value   : the dropout value\n",
    "        TheModel        : The pretrained model loaded,\n",
    "    Outputs\n",
    "        accuracy        : The accuracy of the model\n",
    "    '''\n",
    "    # load pretrained model and add layers on top of the model\n",
    "    x = TheModel.output\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_value)(x)\n",
    "    predictions = Dense(4, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs = TheModel.input, outputs = predictions)\n",
    "    \n",
    "    # freeze base layers for training\n",
    "    for layer in TheModel.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    step_size_train=train_generator.n//train_generator.batch_size\n",
    "    # fit the model\n",
    "    r = model.fit_generator(generator=train_generator,\n",
    "                        validation_data=val_generator,\n",
    "                        steps_per_epoch=step_size_train,\n",
    "                        epochs=epochs)\n",
    "    # valuate the model\n",
    "    scores = model.evaluate(val_generator)\n",
    "    accuracy = scores[1]*100\n",
    "    \n",
    "    return accuracy, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mN111XHxT_1"
   },
   "source": [
    "# **Load and generate data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0_Tnzy7TxT_2",
    "outputId": "30951471-b386-46e1-bf08-0b4193c2497f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2870 images belonging to 4 classes.\n",
      "Found 2870 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "'''\n",
    "Initial and constant parameters for the model\n",
    "initial parameters\n",
    "       batch_size    : the initial batch size we will create our base model with\n",
    "Constant parameters\n",
    "       target_size   : Constant image target size for the whole notebook which is (224,224)\n",
    "       class_mode    : class mode we will use for the whole notebook, which is 'categorical'\n",
    "'''\n",
    "# constant parameters\n",
    "targetsize_constant = (224,224)\n",
    "classmode_constant = 'categorical'\n",
    "\n",
    "# initial parameters\n",
    "batchsize_initial = 8\n",
    "epochs_initial = 4\n",
    "dropout_value_initial = 0.5\n",
    "optimizer_initial = 'Adam'\n",
    "\n",
    "# Data paths\n",
    "train_dataset_path = 'Dataset/Training/'\n",
    "validation_dataset_path = 'Dataset/Validation/'\n",
    "test_dataset_path = 'Dataset/Testing/'\n",
    "\n",
    "\n",
    "train_generator_initial = train_datagen.flow_from_directory(train_dataset_path, # this is where you specify the path to the main data folder\n",
    "                                                 target_size=targetsize_constant,\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=batchsize_initial,\n",
    "                                                 class_mode=classmode_constant,\n",
    "                                                 shuffle=True)\n",
    "val_generator_initial = val_datagen.flow_from_directory(validation_dataset_path, # this is where you specify the path to the main data folder\n",
    "                                                 target_size=targetsize_constant,\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=batchsize_initial,\n",
    "                                                 class_mode=classmode_constant,\n",
    "                                                 shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aq8czHBCxT_3",
    "outputId": "2eb4160d-c972-4b79-bdca-dd840ea4652f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'glioma_tumor': 0, 'meningioma_tumor': 1, 'no_tumor': 2, 'pituitary_tumor': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display classes found in the data\n",
    "train_generator_initial.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1yrduCMxT_3"
   },
   "source": [
    "# **Import the EfficientnetB7 base model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "o2Xs9I5LxT_3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "57exJ3OtxT_3"
   },
   "outputs": [],
   "source": [
    "image_size = [224,224] # choose image size\n",
    "# import the base model\n",
    "efnB7 = tf.keras.applications.efficientnet.EfficientNetB7(input_shape= image_size+[3],weights='imagenet',include_top=False)\n",
    "\n",
    "# storing the base model in the kernel for later use to avoid loading many times\n",
    "efnB7_basemodel = efnB7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wrIJiAZxT_4"
   },
   "source": [
    "#### Add layers on top of the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dPYfNZkOxT_4",
    "outputId": "d3915800-bd69-407c-d138-df9a90ad5ffc"
   },
   "outputs": [],
   "source": [
    "# initial parameter value\n",
    "dropout_value_initial = 0.1\n",
    "\n",
    "# add layers\n",
    "x = efnB7.output\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(dropout_value_initial)(x)\n",
    "predictions = Dense(4, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs = efnB7.input, outputs = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KgqpcREmxT_5"
   },
   "outputs": [],
   "source": [
    "# freeze base layers for training\n",
    "for layer in efnB7.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDdx8c3bxT_5"
   },
   "source": [
    "# **Compile the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vDtNvzVVxT_5",
    "outputId": "ea3fe853-9c4d-4610-b6c2-0c6435d69f7e"
   },
   "outputs": [],
   "source": [
    "# initial optimizer\n",
    "optimizer_initial = 'Adam'\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optimizer_initial,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7pR6exfxT_6"
   },
   "source": [
    "# **Fit the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aPtwNAyMxT_6",
    "outputId": "7c19915d-b144-48f9-bdd8-46292e223d80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ganas\\AppData\\Local\\Temp\\ipykernel_8608\\768208122.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  r = model.fit_generator(generator=train_generator_initial,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "358/358 [==============================] - 3304s 9s/step - loss: 3.0046 - accuracy: 0.5709 - val_loss: 0.8188 - val_accuracy: 0.6812\n",
      "Epoch 2/4\n",
      "358/358 [==============================] - 58746s 165s/step - loss: 0.8639 - accuracy: 0.6782 - val_loss: 0.4725 - val_accuracy: 0.8275\n",
      "Epoch 3/4\n",
      "358/358 [==============================] - 3303s 9s/step - loss: 0.6665 - accuracy: 0.7502 - val_loss: 0.4789 - val_accuracy: 0.8010\n",
      "Epoch 4/4\n",
      "358/358 [==============================] - 3293s 9s/step - loss: 0.6631 - accuracy: 0.7572 - val_loss: 0.4439 - val_accuracy: 0.8303\n",
      "Minutes taken = 1144.150994348526\n"
     ]
    }
   ],
   "source": [
    "# initial parameters\n",
    "epochs_initial = 4\n",
    "step_size_train_initial=train_generator_initial.n//train_generator_initial.batch_size # computing the steps size per epoch\n",
    "\n",
    "tic = time.time()\n",
    "# fit the model\n",
    "r = model.fit_generator(generator=train_generator_initial,\n",
    "                        validation_data=val_generator_initial,\n",
    "                        steps_per_epoch=step_size_train_initial,\n",
    "                        epochs=epochs_initial)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Minutes taken = \" + str((toc-tic)/60.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osNszxOBxT_7"
   },
   "source": [
    "# **Evaluate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8IJCbjp_xT_7",
    "outputId": "17bc486d-1d57-4e4e-af99-e068cf195cc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359/359 [==============================] - 1312s 4s/step - loss: 0.4439 - accuracy: 0.8303\n",
      "evaluate accuracy: 83.03%\n"
     ]
    }
   ],
   "source": [
    "# evaluation on validation data\n",
    "scores = model.evaluate(val_generator_initial)\n",
    "print(\"%s%s: %.2f%%\" % (\"evaluate \",model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lu9VEqIxT_8"
   },
   "source": [
    "# **Section two:** We will start fine tunning\n",
    "\n",
    "## **Tunning optimizers of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "2c71d6e5c6034e9c9b21a522e97b9281"
     ]
    },
    "id": "ZqoG4Cn-xT_8",
    "outputId": "9e47865b-4057-40ca-8d48-15ef8b84f4e3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663e308d62ef4ab79229b0f4faa3afa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ganas\\AppData\\Local\\Temp\\ipykernel_8608\\1030359104.py:80: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  r = model.fit_generator(generator=train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "358/358 [==============================] - 4281s 12s/step - loss: 2.9061 - accuracy: 0.5531 - val_loss: 0.5983 - val_accuracy: 0.7801\n",
      "Epoch 2/4\n",
      "358/358 [==============================] - 5756s 16s/step - loss: 0.9169 - accuracy: 0.6674 - val_loss: 0.5646 - val_accuracy: 0.7878\n",
      "Epoch 3/4\n",
      "358/358 [==============================] - 3960s 11s/step - loss: 0.6793 - accuracy: 0.7449 - val_loss: 0.4450 - val_accuracy: 0.8275\n",
      "Epoch 4/4\n",
      "358/358 [==============================] - 3387s 9s/step - loss: 0.6366 - accuracy: 0.7669 - val_loss: 0.3594 - val_accuracy: 0.8679\n",
      "359/359 [==============================] - 1361s 4s/step - loss: 0.3594 - accuracy: 0.8679\n",
      "Epoch 1/4\n",
      "327/358 [==========================>...] - ETA: 1:40:42 - loss: 1.2483 - accuracy: 0.4946"
     ]
    }
   ],
   "source": [
    "param_label = 'optimizer'\n",
    "param_list = ['Adam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta'] # ['Adam', 'SGD', 'RMSprop', 'Adagrad', 'Adadelta']\n",
    "\n",
    "accuracy_table = {param_label: [], 'accuracy': []}\n",
    "tic = time.time()\n",
    "for param in tqdm.tqdm_notebook(param_list):\n",
    "    # Train, and evaluate model\n",
    "    accuracy, _ = train_evaluate_the_model(train_generator_initial,val_generator_initial,param,epochs_initial,dropout_value_initial,efnB7_basemodel) # replace epochs with 'epochs_initial'\n",
    "    \n",
    "    # Collect results\n",
    "    accuracy_table[param_label].append(param)\n",
    "    accuracy_table['accuracy'].append(accuracy)\n",
    "    \n",
    "accuracy_table = pd.DataFrame(accuracy_table) # convert the table to a dataframe\n",
    "toc = time.time()\n",
    "print(\"Minutes taken = \" + str((toc-tic)/60.0))\n",
    "accuracy_table # display the resullts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTbajds9xT_9",
    "outputId": "65a219ed-40df-4c11-9bec-ac2a70b787e3"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.8.6' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pylab import rcParams\n",
    "\n",
    "# plot results\n",
    "rcParams['figure.figsize'] = 10,8 # width 10, height 8\n",
    "\n",
    "ax = accuracy_table.plot(x='optimizer', y='accuracy',style='bx-', grid=True)\n",
    "ax.set_xlabel(\"Optimizer\")\n",
    "ax.set_ylabel(\"accuracy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjEpuhxMxT_9",
    "outputId": "8a122479-18bb-4986-9147-94df42f34f86"
   },
   "outputs": [],
   "source": [
    "# Get optimum value for param\n",
    "temp = accuracy_table[accuracy_table['accuracy'] == accuracy_table['accuracy'].max()]\n",
    "optimizer_opt = temp[param_label].values[0]\n",
    "print(\"max Accuracy = %0.3f\" % accuracy_table['accuracy'].max())\n",
    "print(\"optimum \" + param_label + \" = \" + str(optimizer_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owlCeUuFxT_-"
   },
   "source": [
    "# **Tunning dropout of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "9db9ef6d0eca4967aec5c952ee09bf8a"
     ]
    },
    "id": "us-U37tsxT_-",
    "outputId": "1b35d7f5-a8f5-4229-e147-8ad84a2129bd"
   },
   "outputs": [],
   "source": [
    "param_label = 'dropout_prob'\n",
    "param_list = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "\n",
    "accuracy_table = {param_label: [], 'accuracy': []}\n",
    "tic = time.time()\n",
    "for param in tqdm.tqdm_notebook(param_list):\n",
    "    # Train, predict and evaluate model\n",
    "    accuracy, _ = train_evaluate_the_model(train_generator_initial,val_generator_initial,optimizer_initial,epochs_initial,param,efnB7_basemodel) # replace epochs with 'epochs_initial'\n",
    "    \n",
    "    # Collect results\n",
    "    accuracy_table[param_label].append(param)\n",
    "    accuracy_table['accuracy'].append(accuracy)\n",
    "    \n",
    "accuracy_table = pd.DataFrame(accuracy_table) # convert the table to a dataframe\n",
    "toc = time.time()\n",
    "print(\"Minutes taken = \" + str((toc-tic)/60.0))\n",
    "accuracy_table # display the resullts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHnRtqZcxT__",
    "outputId": "66c817ce-9985-46bc-cefe-40a730605c17"
   },
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "\n",
    "# plot results\n",
    "rcParams['figure.figsize'] = 10,8 # width 10, height 8\n",
    "\n",
    "ax = accuracy_table.plot(x='dropout_prob', y='accuracy',style='bx-', grid=True)\n",
    "ax.set_xlabel(\"dropout_prob\")\n",
    "ax.set_ylabel(\"accuracy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2Yqm9dTxT__",
    "outputId": "6b3a79bc-eb13-4f1d-a59e-8d0211de928b"
   },
   "outputs": [],
   "source": [
    "# Get optimum value for param\n",
    "temp = accuracy_table[accuracy_table['accuracy'] == accuracy_table['accuracy'].max()]\n",
    "dropout_opt = temp[param_label].values[0]\n",
    "print(\"max Accuracy = %0.3f\" % accuracy_table['accuracy'].max())\n",
    "print(\"optimum \" + param_label + \" = \" + str(dropout_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBMp1AzIxT__"
   },
   "source": [
    "# **Tuning batch size and epochs of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "a8d0338a9bdf4222aa4371310e2608e6",
      "6071ca601d6e42d496136261a15c67b7",
      "c00e239f2ae44be5b9c6b9f09812ec23",
      "2a74df1067de4f5b9830caea9263a887",
      "0af777a103a24587a66ee43fae1c02f2"
     ]
    },
    "id": "dWhKXq5ZxT__",
    "outputId": "15745200-5747-4c83-c3d0-84f5633eadc0"
   },
   "outputs": [],
   "source": [
    "param_label = 'epochs'\n",
    "param_list = [1,2,5,10] # [5, 10, 15, 20]\n",
    "\n",
    "param2_label = 'batch_size'\n",
    "param2_list = [8, 16] # [8, 16, 32] am using this batch sizes for now until we fix the accuracy issue, proper batch sizes to be use [8, 16, 32, 64, 128]\n",
    "\n",
    "accuracy_table = {param_label: [], param2_label: [], 'accuracy': []}\n",
    "tic = time.time()\n",
    "for param in tqdm.tqdm_notebook(param_list):\n",
    "    for param2 in tqdm_notebook(param2_list):\n",
    "        \n",
    "        # generate train and validation data\n",
    "        train_generator,val_generator, _ = get_data_generator(train_dataset_path,validation_dataset_path,test_dataset_path, targetsize_constant, classmode_constant, param2)\n",
    "        # train, predict and evaluate model\n",
    "        accuracy, _ = train_evaluate_the_model(train_generator, val_generator, optimizer_initial, param,dropout_value_initial, efnB7_basemodel)\n",
    "        \n",
    "        # collect results\n",
    "        accuracy_table[param_label].append(param)\n",
    "        accuracy_table[param2_label].append(param2)\n",
    "        accuracy_table['accuracy'].append(accuracy)\n",
    "        \n",
    "accuracy_table = pd.DataFrame(accuracy_table) # convert the table to a dataframe\n",
    "toc = time.time()\n",
    "print(\"Minutes taken = \" + str((toc-tic)/60.0))\n",
    "accuracy_table # display the resullts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0LAh46BxUAA",
    "outputId": "a08adea9-1f96-4e8b-ab99-38a49fb38991"
   },
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "\n",
    "# Plot performance versus params\n",
    "rcParams['figure.figsize'] = 10, 8 # width 10, height 8\n",
    "temp = accuracy_table[accuracy_table[param2_label]==param2_list[0]]\n",
    "ax = temp.plot(x=param_label, y='accuracy', style='bs-', grid=True)\n",
    "legend_list = [param2_label + '_' + str(param2_list[0])]\n",
    "\n",
    "color_list = ['r', 'g', 'k', '0.75']\n",
    "for i in range(1,len(param2_list)):\n",
    "    temp = accuracy_table[accuracy_table[param2_label]==param2_list[i]]\n",
    "    ax = temp.plot(x=param_label, y='accuracy', color=color_list[i%len(color_list)], marker='s', grid=True, ax=ax)\n",
    "    legend_list.append(param2_label + '_' + str(param2_list[i]))\n",
    "\n",
    "ax.set_xlabel(param_label)\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.legend(legend_list, loc='center left', bbox_to_anchor=(1.0, 0.5)) # positions legend outside figure\n",
    "# ax.set_xlim([10, 50])\n",
    "# ax.set_ylim([0, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qlkdu0JQxUAA",
    "outputId": "bc986a0f-fd17-493a-b806-aa4771d3408b"
   },
   "outputs": [],
   "source": [
    "# Get optimum value for param and param2\n",
    "temp = accuracy_table[accuracy_table['accuracy'] == accuracy_table['accuracy'].max()]\n",
    "epochs_opt = temp[param_label].values[0]\n",
    "batch_size_opt = temp[param2_label].values[0]\n",
    "print(\"max Accuracy = %0.3f\" % accuracy_table['accuracy'].max())\n",
    "print(\"optimum \" + param_label + \" = \" + str(epochs_opt))\n",
    "print(\"optimum \" + param2_label + \" = \" + str(batch_size_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqTACpQjxUAA"
   },
   "outputs": [],
   "source": [
    "######\n",
    "dropout_opt = 0.6\n",
    "batch_size_opt = 16\n",
    "epochs_opt = 10\n",
    "optimizer_opt = 'Adagrad'\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5-aHolTxUAB"
   },
   "source": [
    "# **Collecting all optimum tunned parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MgPyY-2xUAB",
    "outputId": "2591b7ab-c500-42ef-b95a-a7e1bf41bfa3"
   },
   "outputs": [],
   "source": [
    "d = {'param': ['optimizer', 'epochs', 'batch_size','dropout_prob'],\n",
    "     'original': [optimizer_initial, epochs_initial, batchsize_initial,dropout_value_initial],\n",
    "     'after_tuning': [optimizer_opt, epochs_opt, batch_size_opt,dropout_opt]}\n",
    "tuned_params = pd.DataFrame(d)\n",
    "tuned_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBqxd5P9xUAB"
   },
   "source": [
    "# **Creating the final model with optimum parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wb-i_CauxUAB",
    "outputId": "fa3516f8-1104-4288-eca1-24298caf85fe"
   },
   "outputs": [],
   "source": [
    "# get train and validation data generator\n",
    "train_generator_opt,val_generator_opt,test_generator_opt = get_data_generator(train_dataset_path,validation_dataset_path,test_dataset_path, targetsize_constant, classmode_constant, batch_size_opt)\n",
    "\n",
    "# train and evaluate the model\n",
    "accuracy, New_model = train_evaluate_the_model(train_generator_opt, val_generator_opt, optimizer_opt, epochs_opt,dropout_opt , efnB7_basemodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BDAbnUyIxUAB",
    "outputId": "140885f4-753b-43c3-8fc0-24556fb6b7ff"
   },
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrH5ed3XxUAB"
   },
   "source": [
    "# **Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8ddcOqPxUAB",
    "outputId": "bc910bb8-f342-4596-e68e-8b318a5facee"
   },
   "outputs": [],
   "source": [
    "New_model.save('brain_tumor_classification_efnB7_test10091335.hdf5') #28060904 - dd/mm/hours/minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPi3rWtexUAC"
   },
   "source": [
    "# ---------------------------------------- END OF NOTEBOOK -----------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9IWjqAzxUAC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FF_EfficientNet Model - Brain Tumor Classification-Final + Model Creation and Fine Tunning  (2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
